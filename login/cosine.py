# -*- coding: utf-8 -*-
"""Cosine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15GbCMwu0kjovs61klha_0mcrmFExikDI
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install nltk
# %pip install scikit-learn
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# from jaccard_index.jaccard import jaccard_index
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import FunctionTransformer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy.spatial.distance import hamming
import string
from scipy.spatial import distance

import numpy as np
import math
import re
from collections import Counter

standard_answer=[]
sample_answer=[]


path = input("Enter the path of the Standard answer text file:")
path1 = input("Enter the path of the Sample answer text file:")


# def get_cosine(vec1, vec2):
#     intersection = set(vec1.keys()) & set(vec2.keys())
#     numerator = sum([vec1[x] * vec2[x] for x in intersection])

#     sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
#     sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])
#     denominator = math.sqrt(sum1) * math.sqrt(sum2)

#     if not denominator:
#         return 0.0
#     else:
#         return float(numerator) / denominator

with open(path, 'r+') as file:
  with open(path1, 'r+') as file1:
    for i in range(10):

         try:

          inp = file.readline()
          inp1= file1.readline()
       
         
          #tokenizing and remove stopword from Standard Answer
          tokens = word_tokenize(inp.lower())
          tokens = [token for token in tokens if token not in string.punctuation]
          stop_words = set(stopwords.words('english'))
          tokens = [token for token in tokens if token not in stop_words]
          processed_text = ' '.join(tokens)
          standard_answer.insert(i,processed_text)
           
          #  #tokenizing and remove stopword from Sample answer
          
          tokens1 = word_tokenize(inp1.lower())
          tokens1 = [token for token in tokens1 if token not in string.punctuation]
          stop_words1 = set(stopwords.words('english'))
          tokens1 = [token for token in tokens1 if token not in stop_words]
          processed_text1 = ' '.join(tokens1)
          sample_answer.insert(i,processed_text1)
          
          print("The answer",i+1 ,"after pre-processing:",processed_text1)

          vectorizer = TfidfVectorizer()

          tfidf_matrix = vectorizer.fit_transform([standard_answer[i], sample_answer[i]])
    
          # Calculate the Hamming distance
          hamming_distance = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])
          
          # Calculate the similarity score
          similarity_score = hamming_distance
          # similarity_percentage = similarity_score * 100
          print("The similarity of question",i+1,":",similarity_score)
          # print("The similarity of question",i+1,":",similarity_percantage)
          print("\n")
         
          # if 0.0 < similarity_score < 1.0:
          #   print("Partially right")

         except Exception as e:
            print(e)