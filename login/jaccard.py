# -*- coding: utf-8 -*-
"""Copy of Jaccard Similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bGMtCTMJ6Z4ZFYkpISxEJ2q9kjU4-2Qf
"""

# import nltk
# nltk.download('punkt')
# nltk.download('stopwords')

# Commented out IPython magic to ensure Python compatibility.
# %pip install jaccard-index

# from jaccard_index.jaccard import jaccard_index
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import FunctionTransformer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy.spatial.distance import hamming
import string
from scipy.spatial import distance

standard_answer = []
sample_answer = []


path = input("Enter the path of the Standard answer text file:")
path1 = input("Enter the path of the Sample answer text file:")


def jaccard_similarity(a, b):
    # convert to set
    a = set(a)
    b = set(b)
    # calucate jaccard similarity
    j = float(len(a.intersection(b))) / len(a.union(b))
    return j


with open(path, 'r+') as file:
    with open(path1, 'r+') as file1:
        for i in range(10):

            try:

                inp = file.readline()
                inp1 = file1.readline()

                # tokenizing and remove stopword from Standard Answer
                tokens = word_tokenize(inp.lower())
                tokens = [
                    token for token in tokens if token not in string.punctuation]
                stop_words = set(stopwords.words('english'))
                tokens = [token for token in tokens if token not in stop_words]
                processed_text = ' '.join(tokens)
                standard_answer.insert(i, processed_text)

                #  #tokenizing and remove stopword from Sample answer

                tokens1 = word_tokenize(inp1.lower())
                tokens1 = [
                    token for token in tokens1 if token not in string.punctuation]
                stop_words1 = set(stopwords.words('english'))
                tokens1 = [
                    token for token in tokens1 if token not in stop_words]
                processed_text1 = ' '.join(tokens1)
                sample_answer.insert(i, processed_text1)

                print("The answer", i+1, "after pre-processing:", processed_text1)

                vectorizer = TfidfVectorizer()

                tfidf_matrix = vectorizer.fit_transform(
                    [standard_answer[i], sample_answer[i]])

                # Calculate the Jaccard similarity
                hamming_distance = jaccard_similarity(
                    tfidf_matrix[0].toarray().flatten(), tfidf_matrix[1].toarray().flatten())

                # Calculate the similarity score
                similarity_score = 1 - hamming_distance
                similarity_percentage = similarity_score * 100
                print("The similarity of question", i+1, ":", similarity_score)
                # print("The similarity of question",i+1,":",similarity_percantage)

                print("\n")

            except Exception as e:
                print(e)


# partially_right = 0
# right = 0
# wrong = 0

# if 0.0 < similarity_score < 1.0 :
#     partially_right+= 1
#     print(partially_right)
# elif similarity_score ==1.0:
#     right+= 1
#     print(right)
# elif similarity_score ==0:
#     wrong+= 1
#     print(wrong)
